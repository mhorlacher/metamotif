{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marc/projects/metamotif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:51:14.113151: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:14.113172: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base2int = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "def sequence2int(sequence):\n",
    "    return [base2int.get(base, 999) for base in sequence]\n",
    "\n",
    "def sequence2onehot(sequence):\n",
    "    return tf.one_hot(sequence2int(sequence), depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kmers(tsv):\n",
    "    kmers_onehot = []\n",
    "    with open(tsv) as f:\n",
    "        _ = f.readline()\n",
    "        for line in f:\n",
    "            name, kmer, score = line.strip().split('\\t')\n",
    "            kmers_onehot.append(sequence2onehot(kmer))\n",
    "    return np.stack(kmers_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 17:51:15.542596: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-07 17:51:15.543311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-06-07 17:51:15.591541: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 17:51:15.591841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3c:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 52.21GiB/s\n",
      "2022-06-07 17:51:15.591936: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:15.591990: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:15.592040: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:15.592089: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:15.592137: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:15.592187: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:15.592243: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:15.592316: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-07 17:51:15.592330: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-07 17:51:15.598243: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-07 17:51:15.598639: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-07 17:51:15.598671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-06-07 17:51:15.598680: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    }
   ],
   "source": [
    "kmers_onehot = load_kmers('examples/QKI_HepG2.5mers.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwm = kmers_onehot[1]\n",
    "pwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metamotif.similarity import motif_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.61 ms ± 147 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "motif_similarity(pwm, pwm, min_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_similarity_to_reference(pwm, reference, reduce=tf.reduce_max, **kwargs):\n",
    "    \"\"\"Compute the similarity between a PWM and a (list) of reference PWMs. \n",
    "    \n",
    "    Similarity between the PWM and a single reference is computed via motif_similarity, \n",
    "    using provided kwargs. Similarity values between the PWM and all references are reduce via a \n",
    "    reduction function (default: max). \n",
    "\n",
    "    Args:\n",
    "        pwm (tf.Tensor): PWM. \n",
    "        reference (tf.Tensor or list): (List of) reference PWMs.\n",
    "        reduce (function, optional): Reduce function. Defaults to tf.reduce_max.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Scalar of similarity to references. \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(reference, list):\n",
    "        pass\n",
    "    else:\n",
    "        reference = [reference]\n",
    "    return reduce([motif_similarity(pwm, pwm_r, **kwargs) for pwm_r in reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_similarity_to_reference(pwm, [pwm, pwm, pwm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 5, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmers_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwm = kmers_onehot[1]\n",
    "pwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwm_pad = tf.pad(pwm, [[2, 2,], [0, 0]], 'CONSTANT')\n",
    "pwm_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor([0. 0. 1. 1. 1.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(pwm_pad[0:5])\n",
    "print(tf.reduce_sum(pwm_pad[0:5], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.slice(pwm_pad, [0, 0], [5, 4])\n",
    "\n",
    "# tf.slice(pwm_pad, tf.range(pwm_pad.shape[0] - 5 + 1), [5, pwm_pad.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 8), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.tile(pwm_pad, [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       "array([[0., 0.],\n",
       "       [0., 0.],\n",
       "       [1., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strided_slice(pwm_pad, [0, 0], [5, 4], strides=[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 4)\n",
      "tf.Tensor(\n",
      "[[[0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0.]]], shape=(5, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pwm_pad_windows = tf.squeeze(np.lib.stride_tricks.sliding_window_view(pwm_pad, window_shape=(5, 4), axis=None))\n",
    "print(pwm_pad_windows.shape)\n",
    "print(pwm_pad_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 4)\n",
      "tf.Tensor(\n",
      "[[[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0.]]], shape=(5, 5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "pwm_tiled = tf.reshape(tf.tile(pwm, [5, 1]), (5, 5, 4))\n",
    "print(pwm_tiled.shape)\n",
    "print(pwm_tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kld = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kld(pwm_tiled, pwm_tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[0., 0., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = tf.cast(tf.logical_and(tf.cast(np.tril(np.ones((5, 5)), k=2), tf.bool), tf.cast(np.tril(np.ones((5, 5)), k=2).transpose(), tf.bool)), tf.float32)\n",
    "mask = mask[::-1]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kld = tf.keras.losses.KLDivergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.stack([pwm_tiled, pwm_tiled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 4), dtype=float32, numpy=\n",
       "array([[0.17030501, 0.76096654, 0.17000723, 0.52600634],\n",
       "       [0.5627916 , 0.34585357, 0.67985904, 0.63700044],\n",
       "       [0.4439932 , 0.89256203, 0.58029795, 0.6968864 ],\n",
       "       [0.0454545 , 0.6419865 , 0.603287  , 0.6360676 ],\n",
       "       [0.28756404, 0.9147514 , 0.29791713, 0.7187692 ]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.uniform(shape=(5, 4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[0.4439932 , 0.89256203, 0.58029795, 0.6968864 ],\n",
       "       [0.0454545 , 0.6419865 , 0.603287  , 0.6360676 ]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.boolean_mask(a, tf.cast([0, 0, 1, 1, 0], tf.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def log(x, basis=None):\n",
    "    if basis is None:\n",
    "        return tf.math.log(x)\n",
    "    else:\n",
    "        return tf.math.log(x) / tf.math.log(tf.cast(basis, x.dtype))\n",
    "\n",
    "@tf.function\n",
    "def kld(q, p, basis=None):\n",
    "    p = tf.convert_to_tensor(p)\n",
    "    q = tf.cast(q, p.dtype)\n",
    "    q = tf.keras.backend.clip(q, tf.keras.backend.epsilon(), 1)\n",
    "    p = tf.keras.backend.clip(p, tf.keras.backend.epsilon(), 1)\n",
    "    return tf.reduce_sum(q * log(q / p, basis=basis), axis=-1)\n",
    "\n",
    "@tf.function\n",
    "def jsd(p, q, logits=True):\n",
    "    m = (p + q) / 2\n",
    "    return 1 - (kld(p, m, basis=2)/2 + kld(q, m, basis=2)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2.1755695e-06 2.5000161e-01 1.0000000e+00 2.5000161e-01 2.1755695e-06], shape=(5,), dtype=float32)\n",
      "tf.Tensor(2.1755695e-06, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function()\n",
    "def motif_point_similarity(pwm_1, pwm_2, boolean_mask=None, weights=None, sim_fn=jsd):\n",
    "    tf.debugging.assert_equal(pwm_1.shape, pwm_1.shape)\n",
    "    \n",
    "    sim = sim_fn(pwm_1, pwm_2)\n",
    "    if weights is not None:\n",
    "        sim = tf.multiply(sim, weights)\n",
    "    if boolean_mask is not None:\n",
    "        sim = tf.boolean_mask(sim, tf.cast(boolean_mask, tf.bool))\n",
    "    return tf.reduce_mean(sim)\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def tile_pwm(pwm, n):\n",
    "    #tf.debugging.assert_rank(pwm, 2)\n",
    "    return tf.reshape(tf.tile(pwm, tf.constant([n, 1], dtype=tf.int64)), (n, pwm.shape[0], pwm.shape[1]))\n",
    "\n",
    "def sliding_window_view(x, window_size):\n",
    "    windows = np.zeros(shape = (x.shape[0] - window_size + 1, window_size, *x.shape[1:]))\n",
    "    for i in range(windows.shape[0]):\n",
    "        windows[i, ] = x[i:(i+window_size), ]\n",
    "    return windows\n",
    "\n",
    "@tf.function\n",
    "def pwm_padded_windows(pwm, padding=0):\n",
    "    pwm_pad = tf.pad(pwm, [[padding, padding,], [0, 0]], 'CONSTANT')\n",
    "    return tf.py_function(func=sliding_window_view, inp=[pwm_pad, pwm.shape[0]], Tout=tf.float32)\n",
    "    #return sliding_window_view(pwm_pad, window_size=5)\n",
    "    #return tf.squeeze(np_sliding_window_view(pwm_pad, window_shape=(5, 4), axis=None))\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def _make_mask(pwm_1, pwm_2):\n",
    "    return tf.cast(tf.logical_and(tf.cast(tf.reduce_sum(pwm_1, axis=1), tf.bool), tf.cast(tf.reduce_sum(pwm_2, axis=1), tf.bool)), tf.float32)\n",
    "\n",
    "@tf.function()\n",
    "def _map_masked_point_similarity(pwm_a_b):\n",
    "    pwm_a, pwm_b = pwm_a_b[0], pwm_a_b[1]\n",
    "    mask = _make_mask(pwm_a, pwm_b)\n",
    "    return motif_point_similarity(pwm_a, pwm_b, boolean_mask=mask)\n",
    "\n",
    "#@tf.function()\n",
    "def motif_similarity(pwm_1, pwm_2, min_size=3, reduce=tf.reduce_max):\n",
    "    # assign larger PWM to pwm_1\n",
    "    if pwm_1.shape[0] < pwm_2.shape[0]:\n",
    "        pwm_1, pwm_2 = pwm_2, pwm_1\n",
    "    \n",
    "    # pad the longer PWM and create sliding windows over it\n",
    "    pwm_1_padded_windows = pwm_padded_windows(pwm_1, padding=(pwm_2.shape[0] - min_size))\n",
    "    #print(pwm_1_padded_windows[0])\n",
    "    \n",
    "    # tile the shorted PWM to match the number of sliding windows\n",
    "    pwm_2_tiled = tile_pwm(pwm_2, pwm_1_padded_windows.shape[0])\n",
    "    #print(pwm_2_tiled[0])\n",
    "\n",
    "    # compute the dinstance between pwm_2 and all windows of pwm_1\n",
    "    window_sims = tf.map_fn(_map_masked_point_similarity, tf.stack([pwm_1_padded_windows, pwm_2_tiled], axis=1))\n",
    "\n",
    "    return window_sims\n",
    "    return reduce(window_sims)\n",
    "\n",
    "#print(_tile_pwm(pwm, n=3))\n",
    "#print(_pwm_padded_windows(pwm, 2))\n",
    "print(motif_similarity(pwm, pwm, min_size=3))\n",
    "\n",
    "print(motif_point_similarity(pwm_pad_windows[0], pwm, boolean_mask=[0,0,1,1,1]))\n",
    "# @tf.function()\n",
    "# def motif_sim(pwm_a, pwm_b, position_weights=None, sim=kld):\n",
    "#     return tf.map_fn(_map_sim, [pwm_a, pwm_b], dtype=tf.float32)\n",
    "\n",
    "# motif_point_similarity(pwm, pwm)\n",
    "\n",
    "# tf.map_fn(_map_masked_point_similarity, tf.stack([pwm_tiled, pwm_tiled], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm_pad_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=9.6708555>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_point_similarity(pwm_pad_windows[4], pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([2.1755695e-06, 2.1755695e-06, 2.1755695e-06, 5.0000113e-01,\n",
       "       5.0000113e-01], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsd(pwm_pad_windows[4], pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_view(x, window_size):\n",
    "    windows = np.zeros(shape = (len(x) - window_size + 1, window_size, *x.shape[1:]))\n",
    "    for i in range(windows.shape[0]):\n",
    "        windows[i, :] = x[i:(i+window_size), ]\n",
    "    return windows\n",
    "\n",
    "sliding_window_view(tf.constant(pwm_pad), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window(pwm_pad, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1, 1, *pwm_pad.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.lib.stride_tricks.sliding_window_view(tf.constant(pwm), window_shape=(2, 4), axis=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.tf_function()\n",
    "def sliding_window_view(ndarray, **kwargs):\n",
    "    tf.numpy_function(np.lib.stride_tricks.sliding_window_view(ndarray, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([pwm_tiled, pwm_tiled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(pmw_1, pmw_2):\n",
    "    mask = tf.cast(tf.logical_and(tf.cast(tf.reduce_sum(pmw_1, axis=1), tf.bool), tf.cast(tf.reduce_sum(pmw_2, axis=1), tf.bool)), tf.float32)\n",
    "    sim = tf.reduce_sum(tf.multiply(pmw_1, pmw_2), axis=1)\n",
    "    sim_masked = tf.multiply(sim, mask)\n",
    "    return tf.reduce_sum(sim_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.map_fn(apply_sim, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tf.map_fn(apply_sim, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.numpy()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_point_similarity(pmw_1, pmw_2):\n",
    "    mask = tf.cast(tf.logical_and(tf.cast(tf.reduce_sum(pmw_1, axis=1), tf.bool), tf.cast(tf.reduce_sum(pmw_2, axis=1), tf.bool)), tf.float32)\n",
    "    sim = tf.reduce_sum(tf.multiply(pmw_1, pmw_2), axis=1)\n",
    "    sim_masked = tf.multiply(sim, mask)\n",
    "    return tf.reduce_sum(sim_masked)\n",
    "\n",
    "motif_point_similarity(pwm, pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "motif_point_similarity(pwm, pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = kmers_onehot[0]\n",
    "b = kmers_onehot[9]\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistConv(tf.keras.layers.Conv1D):\n",
    "    def __init__(self):\n",
    "        super(DistConv, self).__init__(self)\n",
    "\n",
    "dc = DistConv()\n",
    "dc(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    padding = int(min([a.shape[0], b.shape[0]])/2)\n",
    "    a_pad = tf.pad(a, [[padding, padding], [0, 0]])\n",
    "\n",
    "    dists = []\n",
    "    for i in range(a_pad.shape[0] - b.shape[0] + 1):\n",
    "        a_pad_loc = a_pad[i:(i+5)]\n",
    "        dists.append(tf.reduce_sum(a_pad_loc * b))\n",
    "    return max(dists).numpy()\n",
    "\n",
    "dist(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pad = tf.pad(a, [[2, 2], [0, 0]])\n",
    "a_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(a_pad.shape[0] - b.shape[0] + 1):\n",
    "    a_pad_loc = a_pad[i:(i+5)]\n",
    "    print(tf.reduce_sum(a_pad_loc * b))\n",
    "\n",
    "a_pad_after = a_pad.numpy()\n",
    "a_pad_after[1:(1 + 5)] = a_pad_after[1:(1 + 5)] + b\n",
    "a_pad_after = a_pad_after / 2\n",
    "a_pad_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alignment():\n",
    "    def __init__(self, a, b=None):\n",
    "        self.pwm = None\n",
    "        self.support = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.KLD(a[0], b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.convolve(a, b, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "convolve2d(a, b, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07d0b395b5f16580a19d9e84ffce013ba04a7070b89ce152a7a103c5c713ce70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('rbpnet-2.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
