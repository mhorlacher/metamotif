{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marc/projects/metamotif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 20:54:18.863182: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:18.863232: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base2int = {'A': 0, 'C': 1, 'G': 2, 'T': 3}\n",
    "\n",
    "def sequence2int(sequence):\n",
    "    return [base2int.get(base, 999) for base in sequence]\n",
    "\n",
    "def sequence2onehot(sequence):\n",
    "    return tf.one_hot(sequence2int(sequence), depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kmers(tsv):\n",
    "    kmers_onehot = []\n",
    "    with open(tsv) as f:\n",
    "        _ = f.readline()\n",
    "        for line in f:\n",
    "            name, kmer, score = line.strip().split('\\t')\n",
    "            kmers_onehot.append(sequence2onehot(kmer))\n",
    "    return np.stack(kmers_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 20:54:20.358396: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-07 20:54:20.359258: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-06-07 20:54:20.391573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 20:54:20.391813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:3c:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1\n",
      "coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 52.21GiB/s\n",
      "2022-06-07 20:54:20.391888: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:20.391934: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:20.391977: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:20.392019: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:20.392061: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:20.392102: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:20.392143: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:20.392184: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:54:20.392191: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-07 20:54:20.393135: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-07 20:54:20.393508: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-07 20:54:20.393530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-06-07 20:54:20.393537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    }
   ],
   "source": [
    "kmers_onehot = load_kmers('examples/RBFOX2_HepG2.5mers.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwm = kmers_onehot[1]\n",
    "pwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metamotif.similarity import motif_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 20:54:28.527359: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-06-07 20:54:28.547162: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 1999965000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.59 ms ± 478 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "motif_similarity(pwm, pwm, min_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_similarity_to_reference(pwm, reference, reduce=tf.reduce_max, **kwargs):\n",
    "    \"\"\"Compute the similarity between a PWM and a (list) of reference PWMs. \n",
    "    \n",
    "    Similarity between the PWM and a single reference is computed via motif_similarity, \n",
    "    using provided kwargs. Similarity values between the PWM and all references are reduce via a \n",
    "    reduction function (default: max). \n",
    "\n",
    "    Args:\n",
    "        pwm (tf.Tensor): PWM. \n",
    "        reference (tf.Tensor or list): (List of) reference PWMs.\n",
    "        reduce (function, optional): Reduce function. Defaults to tf.reduce_max.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: Scalar of similarity to references. \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(reference, list):\n",
    "        pass\n",
    "    else:\n",
    "        reference = [reference]\n",
    "    return reduce([motif_similarity(pwm, pwm_r, **kwargs) for pwm_r in reference])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=1.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_similarity_to_reference(pwm, [pwm, pwm, pwm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm_fn = '/home/marc/Downloads/RBP_PSSMs/CNOT4_gacaga_human_PSSM.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02892607 0.16888298 0.77326488 0.02892607]\n",
      " [0.98456113 0.00514629 0.00514629 0.00514629]\n",
      " [0.00514629 0.91451045 0.07519696 0.00514629]\n",
      " [0.98456113 0.00514629 0.00514629 0.00514629]\n",
      " [0.00514629 0.1841677  0.80553971 0.00514629]\n",
      " [0.98456113 0.00514629 0.00514629 0.00514629]\n",
      " [0.30100821 0.18412659 0.20970436 0.30516084]]\n"
     ]
    }
   ],
   "source": [
    "def load_pwm(fname):\n",
    "    with open(fname) as f:\n",
    "        pwm = list()\n",
    "        header = f.readline()\n",
    "        if header[:2] == 'ID':\n",
    "            _ = f.readline()\n",
    "        for line in f:\n",
    "            row = line.strip().split('\\t')[1:]\n",
    "            if len(row) != 4:\n",
    "                break\n",
    "            pwm.append(list(map(float, row)))\n",
    "        pwm = np.array(pwm)\n",
    "    return pwm\n",
    "\n",
    "pwm = load_pwm(pwm_fn)\n",
    "print(pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm_dir = '/home/marc/Downloads/RBP_PSSMs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_pwms(pwm_dir):\n",
    "    pwms = dict()\n",
    "    for pwm_txt in Path(pwm_dir).glob('*_human_PSSM.txt'):\n",
    "        RBP = pwm_txt.name.split('_')[0]\n",
    "        if RBP not in pwms:\n",
    "            pwms[RBP] = list()\n",
    "        pwms[RBP].append(load_pwm(pwm_txt))\n",
    "    return pwms\n",
    "\n",
    "pwms = load_pwms(pwm_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1.  , 0.  , 0.  , 0.  ],\n",
       "        [0.  , 0.57, 0.  , 0.43],\n",
       "        [0.  , 0.  , 0.  , 1.  ],\n",
       "        [1.  , 0.  , 0.  , 0.  ],\n",
       "        [1.  , 0.  , 0.  , 0.  ],\n",
       "        [0.  , 0.83, 0.  , 0.17]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwms['QKI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kmers(tsv):\n",
    "    kmers_onehot = []\n",
    "    with open(tsv) as f:\n",
    "        _ = f.readline()\n",
    "        for line in f:\n",
    "            name, kmer, score = line.strip().split('\\t')\n",
    "            kmers_onehot.append(sequence2onehot(kmer))\n",
    "    return np.stack(kmers_onehot)\n",
    "\n",
    "QKI_5mers_target = load_kmers('/home/marc/Downloads/5mers_processed/QKI_HepG2/5mer.RBFOX2_HepG2.profile_target.csv')\n",
    "QKI_5mers_control = load_kmers('/home/marc/Downloads/5mers_processed/QKI_HepG2/5mer.QKI_HepG2.profile_control.csv')\n",
    "QKI_5mers_total = load_kmers('/home/marc/Downloads/5mers_processed/QKI_HepG2/5mer.QKI_HepG2.profile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float64, numpy=0.9302546937479207>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motif_similarity_to_reference(QKI_5mers_target[0], pwms['QKI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "QKI_5mers_target_sims = [motif_similarity_to_reference(mer, pwms['QKI'], min_size=3).numpy() for mer in QKI_5mers_target]\n",
    "QKI_5mers_control_sims = [motif_similarity_to_reference(mer, pwms['QKI'], min_size=3).numpy() for mer in QKI_5mers_control]\n",
    "QKI_5mers_total_sims = [motif_similarity_to_reference(mer, pwms['QKI'], min_size=3).numpy() for mer in QKI_5mers_total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9235518389296927\n",
      "0.6644967164812121\n",
      "0.9236690094906977\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(QKI_5mers_target_sims))\n",
    "print(np.mean(QKI_5mers_control_sims))\n",
    "print(np.mean(QKI_5mers_total_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8159882319083178\n",
      "0.6188980017111788\n",
      "0.6628939076290632\n"
     ]
    }
   ],
   "source": [
    "RBFOX2_5mers_target = load_kmers('/home/marc/Downloads/5mers_processed/RBFOX2_HepG2/5mer.RBFOX2_HepG2.profile_target.csv')\n",
    "RBFOX2_5mers_control = load_kmers('/home/marc/Downloads/5mers_processed/RBFOX2_HepG2/5mer.RBFOX2_HepG2.profile_control.csv')\n",
    "RBFOX2_5mers_total = load_kmers('/home/marc/Downloads/5mers_processed/RBFOX2_HepG2/5mer.RBFOX2_HepG2.profile.csv')\n",
    "\n",
    "RBFOX2_5mers_target_sims = [motif_similarity_to_reference(mer, pwms['RBFOX2'], min_size=3).numpy() for mer in RBFOX2_5mers_target]\n",
    "RBFOX2_5mers_control_sims = [motif_similarity_to_reference(mer, pwms['RBFOX2'], min_size=3).numpy() for mer in RBFOX2_5mers_control]\n",
    "RBFOX2_5mers_total_sims = [motif_similarity_to_reference(mer, pwms['RBFOX2'], min_size=3).numpy() for mer in RBFOX2_5mers_total]\n",
    "\n",
    "print(np.mean(RBFOX2_5mers_target_sims))\n",
    "print(np.mean(RBFOX2_5mers_control_sims))\n",
    "print(np.mean(RBFOX2_5mers_total_sims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9235518389296756"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(QKI_5mers_target_sims)/len(QKI_5mers_target_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pwm[0:3])\n",
    "print(pwm[3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metamotif.similarity import motif_point_similarity\n",
    "\n",
    "motif_point_similarity(pwm[0:3], pwm[3:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metamotif.similarity import motif_similarity\n",
    "\n",
    "motif_similarity(pwm[2:5], pwm[3:6], min_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmers_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm = kmers_onehot[1]\n",
    "pwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm_pad = tf.pad(pwm, [[2, 2,], [0, 0]], 'CONSTANT')\n",
    "pwm_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pwm_pad[0:5])\n",
    "print(tf.reduce_sum(pwm_pad[0:5], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.slice(pwm_pad, [0, 0], [5, 4])\n",
    "\n",
    "# tf.slice(pwm_pad, tf.range(pwm_pad.shape[0] - 5 + 1), [5, pwm_pad.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.tile(pwm_pad, [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.strided_slice(pwm_pad, [0, 0], [5, 4], strides=[2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm_pad_windows = tf.squeeze(np.lib.stride_tricks.sliding_window_view(pwm_pad, window_shape=(5, 4), axis=None))\n",
    "print(pwm_pad_windows.shape)\n",
    "print(pwm_pad_windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm_tiled = tf.reshape(tf.tile(pwm, [5, 1]), (5, 5, 4))\n",
    "print(pwm_tiled.shape)\n",
    "print(pwm_tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kld = tf.keras.losses.KLDivergence(reduction=tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kld(pwm_tiled, pwm_tiled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.cast(tf.logical_and(tf.cast(np.tril(np.ones((5, 5)), k=2), tf.bool), tf.cast(np.tril(np.ones((5, 5)), k=2).transpose(), tf.bool)), tf.float32)\n",
    "mask = mask[::-1]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kld = tf.keras.losses.KLDivergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.stack([pwm_tiled, pwm_tiled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform(shape=(5, 4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.boolean_mask(a, tf.cast([0, 0, 1, 1, 0], tf.bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def log(x, basis=None):\n",
    "    if basis is None:\n",
    "        return tf.math.log(x)\n",
    "    else:\n",
    "        return tf.math.log(x) / tf.math.log(tf.cast(basis, x.dtype))\n",
    "\n",
    "@tf.function\n",
    "def kld(q, p, basis=None):\n",
    "    p = tf.convert_to_tensor(p)\n",
    "    q = tf.cast(q, p.dtype)\n",
    "    q = tf.keras.backend.clip(q, tf.keras.backend.epsilon(), 1)\n",
    "    p = tf.keras.backend.clip(p, tf.keras.backend.epsilon(), 1)\n",
    "    return tf.reduce_sum(q * log(q / p, basis=basis), axis=-1)\n",
    "\n",
    "@tf.function\n",
    "def jsd(p, q, logits=True):\n",
    "    m = (p + q) / 2\n",
    "    return 1 - (kld(p, m, basis=2)/2 + kld(q, m, basis=2)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def motif_point_similarity(pwm_1, pwm_2, boolean_mask=None, weights=None, sim_fn=jsd):\n",
    "    tf.debugging.assert_equal(pwm_1.shape, pwm_1.shape)\n",
    "    \n",
    "    sim = sim_fn(pwm_1, pwm_2)\n",
    "    if weights is not None:\n",
    "        sim = tf.multiply(sim, weights)\n",
    "    if boolean_mask is not None:\n",
    "        sim = tf.boolean_mask(sim, tf.cast(boolean_mask, tf.bool))\n",
    "    return tf.reduce_mean(sim)\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def tile_pwm(pwm, n):\n",
    "    #tf.debugging.assert_rank(pwm, 2)\n",
    "    return tf.reshape(tf.tile(pwm, tf.constant([n, 1], dtype=tf.int64)), (n, pwm.shape[0], pwm.shape[1]))\n",
    "\n",
    "def sliding_window_view(x, window_size):\n",
    "    windows = np.zeros(shape = (x.shape[0] - window_size + 1, window_size, *x.shape[1:]))\n",
    "    for i in range(windows.shape[0]):\n",
    "        windows[i, ] = x[i:(i+window_size), ]\n",
    "    return windows\n",
    "\n",
    "@tf.function\n",
    "def pwm_padded_windows(pwm, padding=0):\n",
    "    pwm_pad = tf.pad(pwm, [[padding, padding,], [0, 0]], 'CONSTANT')\n",
    "    return tf.py_function(func=sliding_window_view, inp=[pwm_pad, pwm.shape[0]], Tout=tf.float32)\n",
    "    #return sliding_window_view(pwm_pad, window_size=5)\n",
    "    #return tf.squeeze(np_sliding_window_view(pwm_pad, window_shape=(5, 4), axis=None))\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def _make_mask(pwm_1, pwm_2):\n",
    "    return tf.cast(tf.logical_and(tf.cast(tf.reduce_sum(pwm_1, axis=1), tf.bool), tf.cast(tf.reduce_sum(pwm_2, axis=1), tf.bool)), tf.float32)\n",
    "\n",
    "@tf.function()\n",
    "def _map_masked_point_similarity(pwm_a_b):\n",
    "    pwm_a, pwm_b = pwm_a_b[0], pwm_a_b[1]\n",
    "    mask = _make_mask(pwm_a, pwm_b)\n",
    "    return motif_point_similarity(pwm_a, pwm_b, boolean_mask=mask)\n",
    "\n",
    "#@tf.function()\n",
    "def motif_similarity(pwm_1, pwm_2, min_size=3, reduce=tf.reduce_max):\n",
    "    # assign larger PWM to pwm_1\n",
    "    if pwm_1.shape[0] < pwm_2.shape[0]:\n",
    "        pwm_1, pwm_2 = pwm_2, pwm_1\n",
    "    \n",
    "    # pad the longer PWM and create sliding windows over it\n",
    "    pwm_1_padded_windows = pwm_padded_windows(pwm_1, padding=(pwm_2.shape[0] - min_size))\n",
    "    #print(pwm_1_padded_windows[0])\n",
    "    \n",
    "    # tile the shorted PWM to match the number of sliding windows\n",
    "    pwm_2_tiled = tile_pwm(pwm_2, pwm_1_padded_windows.shape[0])\n",
    "    #print(pwm_2_tiled[0])\n",
    "\n",
    "    # compute the dinstance between pwm_2 and all windows of pwm_1\n",
    "    window_sims = tf.map_fn(_map_masked_point_similarity, tf.stack([pwm_1_padded_windows, pwm_2_tiled], axis=1))\n",
    "\n",
    "    return window_sims\n",
    "    return reduce(window_sims)\n",
    "\n",
    "#print(_tile_pwm(pwm, n=3))\n",
    "#print(_pwm_padded_windows(pwm, 2))\n",
    "print(motif_similarity(pwm, pwm, min_size=3))\n",
    "\n",
    "print(motif_point_similarity(pwm_pad_windows[0], pwm, boolean_mask=[0,0,1,1,1]))\n",
    "# @tf.function()\n",
    "# def motif_sim(pwm_a, pwm_b, position_weights=None, sim=kld):\n",
    "#     return tf.map_fn(_map_sim, [pwm_a, pwm_b], dtype=tf.float32)\n",
    "\n",
    "# motif_point_similarity(pwm, pwm)\n",
    "\n",
    "# tf.map_fn(_map_masked_point_similarity, tf.stack([pwm_tiled, pwm_tiled], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwm_pad_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_point_similarity(pwm_pad_windows[4], pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsd(pwm_pad_windows[4], pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_view(x, window_size):\n",
    "    windows = np.zeros(shape = (len(x) - window_size + 1, window_size, *x.shape[1:]))\n",
    "    for i in range(windows.shape[0]):\n",
    "        windows[i, :] = x[i:(i+window_size), ]\n",
    "    return windows\n",
    "\n",
    "sliding_window_view(tf.constant(pwm_pad), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window(pwm_pad, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1, 1, *pwm_pad.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.lib.stride_tricks.sliding_window_view(tf.constant(pwm), window_shape=(2, 4), axis=None).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.tf_function()\n",
    "def sliding_window_view(ndarray, **kwargs):\n",
    "    tf.numpy_function(np.lib.stride_tricks.sliding_window_view(ndarray, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.stack([pwm_tiled, pwm_tiled], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(pmw_1, pmw_2):\n",
    "    mask = tf.cast(tf.logical_and(tf.cast(tf.reduce_sum(pmw_1, axis=1), tf.bool), tf.cast(tf.reduce_sum(pmw_2, axis=1), tf.bool)), tf.float32)\n",
    "    sim = tf.reduce_sum(tf.multiply(pmw_1, pmw_2), axis=1)\n",
    "    sim_masked = tf.multiply(sim, mask)\n",
    "    return tf.reduce_sum(sim_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.map_fn(apply_sim, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tf.map_fn(apply_sim, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.numpy()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_point_similarity(pmw_1, pmw_2):\n",
    "    mask = tf.cast(tf.logical_and(tf.cast(tf.reduce_sum(pmw_1, axis=1), tf.bool), tf.cast(tf.reduce_sum(pmw_2, axis=1), tf.bool)), tf.float32)\n",
    "    sim = tf.reduce_sum(tf.multiply(pmw_1, pmw_2), axis=1)\n",
    "    sim_masked = tf.multiply(sim, mask)\n",
    "    return tf.reduce_sum(sim_masked)\n",
    "\n",
    "motif_point_similarity(pwm, pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "motif_point_similarity(pwm, pwm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = kmers_onehot[0]\n",
    "b = kmers_onehot[9]\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistConv(tf.keras.layers.Conv1D):\n",
    "    def __init__(self):\n",
    "        super(DistConv, self).__init__(self)\n",
    "\n",
    "dc = DistConv()\n",
    "dc(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    padding = int(min([a.shape[0], b.shape[0]])/2)\n",
    "    a_pad = tf.pad(a, [[padding, padding], [0, 0]])\n",
    "\n",
    "    dists = []\n",
    "    for i in range(a_pad.shape[0] - b.shape[0] + 1):\n",
    "        a_pad_loc = a_pad[i:(i+5)]\n",
    "        dists.append(tf.reduce_sum(a_pad_loc * b))\n",
    "    return max(dists).numpy()\n",
    "\n",
    "dist(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pad = tf.pad(a, [[2, 2], [0, 0]])\n",
    "a_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(a_pad.shape[0] - b.shape[0] + 1):\n",
    "    a_pad_loc = a_pad[i:(i+5)]\n",
    "    print(tf.reduce_sum(a_pad_loc * b))\n",
    "\n",
    "a_pad_after = a_pad.numpy()\n",
    "a_pad_after[1:(1 + 5)] = a_pad_after[1:(1 + 5)] + b\n",
    "a_pad_after = a_pad_after / 2\n",
    "a_pad_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Alignment():\n",
    "    def __init__(self, a, b=None):\n",
    "        self.pwm = None\n",
    "        self.support = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.losses.KLD(a[0], b[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.convolve(a, b, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "convolve2d(a, b, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07d0b395b5f16580a19d9e84ffce013ba04a7070b89ce152a7a103c5c713ce70"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('rbpnet-2.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
